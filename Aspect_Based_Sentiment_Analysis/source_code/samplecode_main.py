# -*- coding: utf-8 -*-
"""SampleCode.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jj3U1eMAulkO1DGREJA1z1WZFzGEMZd5
"""
#!/bin/env python3
import subprocess
subprocess.call(["pip", "install", "torch","torchvision","torchaudio"])
subprocess.call(["pip", "install","stanfordnlp"])
subprocess.call(["pip","install","stanza"])
subprocess.call(["pip","install","langdetect"])

import pandas as pd
import numpy as np
import nltk
import stanfordnlp
import torch

from nltk.corpus import stopwords
from nltk.corpus import wordnet
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem.wordnet import WordNetLemmatizer

stanfordnlp.download('en')
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

import stanza
import csv
stanza.download('en')

from langdetect import detect


def aspect_sentiment_analysis(txt, nlp):

    try:
        txt = txt.lower()  # Lowercasing the given Text
        fcluster = []
        totalfeatureList = []
        finalcluster = []
        dic = {}

        if detect(txt) != 'en':
            print("Language not supported")
            return finalcluster

        # Tokenize the text into sentences
        sentList = nltk.sent_tokenize(txt)
        for line in sentList:
            newtaggedList = []
            txt_list = nltk.word_tokenize(line)  # Splitting up into words
            taggedList = nltk.pos_tag(txt_list)  # Doing Part-of-Speech Tagging to each word

            newwordList = []
            flag = 0
            for i in range(0, len(taggedList) - 1):
                if taggedList[i][1] == "NN" and taggedList[i + 1][1] == "NN":  # If two consecutive words are Nouns then they are joined together
                    newwordList.append(taggedList[i][0] + taggedList[i + 1][0])
                    flag = 1
                else:
                    if flag == 1:
                        flag = 0
                        continue
                    newwordList.append(taggedList[i][0])
                    if i == len(taggedList) - 2:
                        newwordList.append(taggedList[i + 1][0])

            finaltxt = ' '.join(word for word in newwordList)
            new_txt_list = nltk.word_tokenize(finaltxt)
            wordsList = [w for w in new_txt_list if not w in stop_words]
            taggedList = nltk.pos_tag(wordsList)

            # Process the text with Stanza
            doc = nlp(finaltxt)

            # Getting the dependency relations between the words
            dep_node = []

            for sentence in doc.sentences:
                if len(sentence.dependencies) > 0:
                    for dep_edge in sentence.dependencies:
                        dep_node.append([dep_edge[2].text, dep_edge[0].id, dep_edge[1]])
                else:
                    print("No dependencies found for sentence:", sentence.text)

            if len(dep_node) > 0:
                # Converting it into appropriate format
                for i in range(len(dep_node)):
                    if (int(dep_node[i][1]) != 0):
                        dep_node[i][1] = newwordList[int(dep_node[i][1]) - 1]

            featureList = []
            categories = []
            for i in taggedList:
                if i[1] in ['JJ', 'NN', 'JJR', 'NNS', 'RB']:
                    featureList.append(list(i))  # For features for each sentence
                    totalfeatureList.append(list(i))  # Stores the features of all the sentences in the text
                    categories.append(i[0])

            for i in featureList:
                filist = []
                for j in dep_node:
                    if (j[0] == i[0] or j[1] == i[0]) and (j[2] in ["nsubj", "acl:relcl", "obj", "dobj", "agent", "advmod", "amod", "neg", "prep_of", "acomp", "xcomp", "compound"]):
                        if j[0] == i[0]:
                            filist.append(j[1])
                        else:
                            filist.append(j[0])
                fcluster.append([i[0], filist])

        for i in totalfeatureList:
            dic[i[0]] = i[1]

        for i in fcluster:
            if dic[i[0]] == "NN":
                finalcluster.append(i)
    except:
        print('Error')
        return []
    print('Success')
    return finalcluster

# Create Stanza pipeline
nlp = stanza.Pipeline(lang='en')
stop_words = set(stopwords.words('english'))

# Load dataset
dataset = pd.read_csv('reviews_2023.csv', encoding="utf-8")[0:20000]
words_to_replace = ["<br>", "</br>", "\\r","-", "<br/>", "√¢‚Ç¨‚Ñ¢", ":", "/", "!", "üòä", "üëç", "√É¬©", "√É¬≥", "√É"]  # list of invalid characters
pd.options.display.max_colwidth = 100000

for x in words_to_replace:
    dataset["comments"] = dataset.comments.str.replace(str(x), " ")
    dataset["comments"] = dataset.comments.str.replace(r'\s+', ' ', regex=True)


# Perform aspect sentiment analysis for each comment
dataset['finalcluster'] = dataset['comments'].map(lambda x: aspect_sentiment_analysis(x, nlp))

# Save the dataset to a CSV file
dataset.to_csv('output_2023_0_20000.csv')

